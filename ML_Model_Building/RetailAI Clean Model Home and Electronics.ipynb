{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "import sys\n",
        "print(sys.version)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "from pyspark.sql.functions import unix_timestamp\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml.feature import RFormula\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.table(\"retailaidb.cleaned_dataset\")\n",
        "spark.sparkContext.setCheckpointDir('checkpoint/')\n",
        "print(\"First 5 rows:\")\n",
        "print(df.head(5))\n",
        "print(\"Columns: \")\n",
        "print(df.columns)\n",
        "print(\"No. of rows:\", df.count())"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Filter only for Electronics items\n",
        "\n",
        "df = df.withColumn('category_code_new', df['category_code'].substr(0, 11))\n",
        "df = df.filter(\"category_code_new = 'electronics'\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "top_category = df.groupBy('category_code_new').count().sort('count', ascending=False).limit(5) # only keep top 5 categories\n",
        "top_category = top_category.withColumnRenamed(\"category_code\",\"category_code_tmp\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "item_to_save = df.groupBy('product_id', \"category_code\").count().sort('count', ascending=False)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "item_to_save = item_to_save.join(top_category, top_category.category_code_tmp == item_to_save.category_code).limit(20)\n",
        "item_to_save.show(20, False)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "raw_df = df\n",
        "\n",
        "product_count = df.groupBy('product_id').count()\n",
        "product_count = product_count.filter(\"count >= 30000\").orderBy('count', ascending=False) #only counts when the product has 20000 views\n",
        "\n",
        "raw_df = raw_df.withColumnRenamed(\"product_id\",\"product_id_tmp\")\n",
        "raw_df = raw_df.join(product_count, raw_df.product_id_tmp == product_count.product_id)\n",
        "\n",
        "user_count = df.groupBy('user_id').count()\n",
        "user_count = user_count.filter(\"count >= 200\").orderBy('count', ascending=False) #only counts when the user has 100 clicks\n",
        "\n",
        "raw_df = raw_df.withColumnRenamed(\"user_id\",\"user_id_tmp\")\n",
        "raw_df = raw_df.join(user_count, raw_df.user_id_tmp == user_count.user_id)\n",
        "\n",
        "df = raw_df\n",
        "\n",
        "df = df.where(df.event_type == \"view\")\n",
        "df = df.drop(\"event_time\",\"category_code\",\"user_session\",\"price\",\"brand\",\"category_id\")\n",
        "df = df.groupBy([df.product_id, df.user_id]).count()\n",
        "\n",
        "# spark.write.table(\"retailaidb.cleaned_dataset_20000filter\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# save table for further use\n",
        "df.write.saveAsTable(\"retailaidb.cleaned_dataset_electronics\", mode=\"overwrite\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#import the required functions and libraries\n",
        "from pyspark.sql.functions import *"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "df = df.withColumn(\"user_id\", df[\"user_id\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"product_id\", df[\"product_id\"].cast(IntegerType()))\n",
        "df.printSchema()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df.show(10, False)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#split the data into training and test datatset\n",
        "train,test=df.randomSplit([0.75,0.25])\n",
        "print(\"Training Count:\")\n",
        "train.count()\n",
        "print(\"Test Count:\")\n",
        "test.count()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#import ALS recommender function from pyspark ml library\n",
        "from pyspark.ml.recommendation import ALS\n",
        "#Training the recommender model using train datatset\n",
        "rec=ALS(maxIter=40,regParam=0.20,implicitPrefs = True, userCol='user_id',itemCol='product_id',ratingCol='count',nonnegative=True,coldStartStrategy=\"drop\", rank=25)\n",
        "#fit the model on train set\n",
        "rec_model=rec.fit(train)\n",
        "#making predictions on test set \n",
        "predicted_ratings=rec_model.transform(test)\n",
        "#columns in predicted ratings dataframe\n",
        "predicted_ratings.printSchema()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#predicted vs actual ratings for test set \n",
        "predicted_ratings.orderBy(rand()).show(10)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "predicted_ratings_witherr=predicted_ratings.withColumn('err',abs(predicted_ratings[\"prediction\"] - predicted_ratings[\"count\"]))\n",
        "predicted_ratings_witherr.show()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df.groupBy('count').count().orderBy('count',ascending=True).show()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "predicted_ratings_witherr.groupBy('count').agg({'err':'mean'}).orderBy('count',ascending=True).show()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#importing Regression Evaluator to measure RMSE\n",
        "from pyspark.ml.evaluation import RegressionEvaluator"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#create Regressor evaluator object for measuring accuracy\n",
        "evaluator=RegressionEvaluator(metricName='rmse',predictionCol='prediction',labelCol='count')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#apply the RE on predictions dataframe to calculate RMSE\n",
        "rmse=evaluator.evaluate(predicted_ratings)\n",
        "print(rmse)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Save the model\n",
        "rec_model.write().overwrite().save(\"retailai_recommendation_model\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#create function to recommend top 'n' products to any particular user\n",
        "def top_recommendations(user_id,n):\n",
        "    \"\"\"\n",
        "    This function returns the top 'n' products that user has not seen yet but might like \n",
        "    \n",
        "    \"\"\"\n",
        "    #assigning alias name 'a' to unique products df\n",
        "    a = unique_products.alias('a')\n",
        "    \n",
        "    #creating another dataframe which contains already watched product by active user \n",
        "    watched_products=indexed.filter(indexed['user_id'] == user_id).select('product_id')\n",
        "    \n",
        "    #assigning alias name 'b' to watched products df\n",
        "    b=watched_products.alias('b')\n",
        "    \n",
        "    #joining both tables on left join \n",
        "    total_products = a.join(b, a.product_id == b.product_id,how='left')\n",
        "    \n",
        "    #selecting products which active user is yet to rate or watch\n",
        "    remaining_products=total_products.where(col(\"b.product_id\").isNull()).select(a.product_id).distinct()\n",
        "    \n",
        "    \n",
        "    #adding new column of user_Id of active useer to remaining products df \n",
        "    remaining_products=remaining_products.withColumn(\"user_id\",lit(int(user_id)))\n",
        "    \n",
        "    \n",
        "    #making recommendations using ALS recommender model and selecting only top 'n' products\n",
        "    recommendations=rec_model.transform(remaining_products).orderBy('prediction',ascending=False).limit(n)\n",
        "    \n",
        "    \n",
        "    #adding columns of product titles in recommendations\n",
        "    product_title = IndexToString(inputCol=\"product_id\", outputCol=\"product_id\",labels=model.labels)\n",
        "    final_recommendations=product_title.transform(recommendations)\n",
        "    \n",
        "    #return the recommendations to active user\n",
        "    return final_recommendations.show(n,False)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "top_recommendations(522313122,10)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.ml.recommendation import ALSModel\n",
        "model_reload = ALSModel.load(\"retailai_recommendation_model\")\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "# partly borrowed from https://github.com/akshitvjain/item-based-recommender\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "from pyspark.ml.recommendation import ALSModel\n",
        "\n",
        "class ItemBasedRecommender():\n",
        "\n",
        "    def __init__(self, model, spark):\n",
        "        self.model = model\n",
        "        self.spark = spark\n",
        "        self.itemFactors = self.model.itemFactors\n",
        "\n",
        "    def compute_similarity(self, item_id):\n",
        "        item = self.itemFactors.where(\n",
        "            col('id') == item_id).select(col('features'))\n",
        "        item_features = item.rdd.map(lambda x: x.features).first()\n",
        "\n",
        "        lol = []\n",
        "        for row in self.itemFactors.rdd.toLocalIterator():\n",
        "            _id = row.__getattr__('id')\n",
        "            features = row.__getattr__('features')\n",
        "            similarity_score = self._cosine_similarity(features, item_features)\n",
        "            if _id != item_id:\n",
        "                lol.append([_id, similarity_score])\n",
        "\n",
        "        R = Row('item_index', 'similarity_score')\n",
        "        self.similar_items_df = self.spark.createDataFrame(\n",
        "            [R(col[0], float(col[1])) for col in lol])\n",
        "        self.similar_items_df = self.similar_items_df.orderBy(\n",
        "            col('similarity_score').desc()).na.drop()\n",
        "        return self.similar_items_df\n",
        "\n",
        "\n",
        "    def _cosine_similarity(self, vector_1, vector_2):\n",
        "        v1 = np.asarray(vector_1)\n",
        "        v2 = np.asarray(vector_2)\n",
        "        cs = v1.dot(v2) / (LA.norm(v1) * LA.norm(v2))\n",
        "        return(cs)\n",
        "\n",
        "# item id as input. Note: this is the normalized item id starting from 0.        \n",
        "test_id = 100\n",
        "item_based_rec = ItemBasedRecommender(model_reload, spark)\n",
        "ret_df = item_based_rec.compute_similarity(test_id)\n",
        "ret_df.show(5)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        ""
      ],
      "attachments": {}
    }
  ],
  "metadata": {
    "description": "xiaoyong's test notebook",
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}